_target_: src.models.ipadapter_module.IPAdapterLitModule

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-5

# scheduler:
#   _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#   _partial_: true
#   mode: min
#   factor: 0.1
#   patience: 10

image_proj_model:
  _target_: src.models.components.image_proj_model.ImageProjModel
  cross_attention_dim: 1024
  clip_embeddings_dim: 512
  clip_extra_context_tokens: 4

text_mask_attn:
  _target_: src.models.components.attention.CrossAttention
  query_dim: 1024
  context_dim: 1024

mask_text_attn:
  _target_: src.models.components.attention.CrossAttention
  query_dim: 1024
  context_dim: 1024



lr_scheduler: constant

unet: checkpoints/sd_turbo
vae: checkpoints/sd_turbo
text_encoder: checkpoints/sd_turbo
face_seg:
  _target_: facer.face_parsing.FaRLFaceParser
  conf_name: farl/celebm/448
  model_path: checkpoints/face/face_parsing.farl.celebm.main_ema_181500_jit.pt
diffusion_schedule: checkpoints/sd_turbo
mask_encoder:
  _target_: src.models.components.mask_encoder.MaskEncoder
  num_embeddings: 19
  embedding_dim: 3
  image_resolution: 224
  vision_patch_size: 16
  vision_width: 768
  vision_layers: 12
  embed_dim: 512
mask_encoder_weight_path: logs/train/runs/mask_clip/2024-12-29_00-24-22/checkpoints/epoch=031-val_loss=0.0398.ckpt
froze_components: ["vae","text_encoder","unet","net_clip","face_seg","mask_encoder"]
loss_type: ["mse_loss"]
# compile model for faster training with pytorch 2.0
compile: false
