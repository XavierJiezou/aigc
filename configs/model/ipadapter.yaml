_target_: src.models.ipadapter_module.IPAdapterLitModule

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-5

# scheduler:
#   _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#   _partial_: true
#   mode: min
#   factor: 0.1
#   patience: 10

image_proj_model:
  _target_: src.models.components.image_proj_model.ImageProjModel
  cross_attention_dim: 768
  clip_embeddings_dim: 768
  clip_extra_context_tokens: 4

text_mask_attn:
  _target_: src.models.components.attention.CrossAttention
  query_dim: 768
  context_dim: 768

mask_text_attn:
  _target_: src.models.components.attention.CrossAttention
  query_dim: 768
  context_dim: 768



lr_scheduler: constant

unet: checkpoints/stablev15
vae: checkpoints/stablev15
text_encoder: checkpoints/stablev15
image_encoder: checkpoints/clip_large_patch14
froze_components: ["vae","text_encoder","unet","image_encoder"]
diffusion_schedule: checkpoints/stablev15
# compile model for faster training with pytorch 2.0
compile: false
