_target_: src.models.stable_diffusion_lora_module.StableDiffusionLoraLitModule

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-4
  betas: [0.9,0.999]
  weight_decay: 1e-2
  eps: 1e-8

# scheduler:
#   _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#   _partial_: true
#   mode: min
#   factor: 0.1
#   patience: 10

scheduler: null

unet: checkpoints/stablev15
vae: checkpoints/stablev15
text_encoder: checkpoints/stablev15
diffusion_schedule: checkpoints/stablev15
lora_config:
  _target_: peft.LoraConfig
  r: 4
  lora_alpha: 8              # r * 2
  init_lora_weights: "gaussian"
  target_modules: ["to_k", "to_q", "to_v", "to_out.0"]
froze_components: ["vae","text_encoder"]
# compile model for faster training with pytorch 2.0
compile: false
