_target_: src.models.stable_diffusion_lora_mask_module.StableDiffusionLoraMaskLitModule

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-4
  betas: [0.9,0.999]
  weight_decay: 1e-2
  eps: 1e-8

projection:
  _target_: src.models.components.proj.Proj
  embed_dim: 768
  hidden_dim: 32

# scheduler:
#   _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#   _partial_: true
#   mode: min
#   factor: 0.1
#   patience: 10

mask_encoder:
  _target_: src.models.components.mask_encoder_spi.MaskEncoder
  num_embeddings: 19
  embedding_dim: 3
  image_resolution: 224
  vision_patch_size: 16
  vision_width: 768
  vision_layers: 12
  embed_dim: 512
mask_encoder_weight_path: logs/train/runs/mask_clip/2024-12-29_00-24-22/checkpoints/epoch=031-val_loss=0.0398.ckpt

scheduler: null

unet: checkpoints/stablev15
vae: checkpoints/stablev15
text_encoder: checkpoints/stablev15
diffusion_schedule: checkpoints/stablev15
lora_config:
  _target_: peft.LoraConfig
  r: 4
  lora_alpha: 8              # r * 2
  init_lora_weights: "gaussian"
  target_modules: ["to_k", "to_q", "to_v", "to_out.0"]
froze_components: ["vae","text_encoder","mask_encoder"]
# compile model for faster training with pytorch 2.0
compile: false
