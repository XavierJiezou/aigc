_target_: src.models.var_module.VARLitModule

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10

var:
  _target_: src.models.var.var.VAR
  vae_local:
    _target_: src.models.var.vqvae.VQVAE
    vocab_size: 4096
    z_channels: 32
    ch: 160
    test_mode: true
    share_quant_resi: 4
    v_patch_nums: [1, 2, 3, 4, 5, 6, 8, 10, 13, 16]
  num_classes: 10
  depth: 16
  embed_dim: 256
  num_heads: 16
  drop_rate: 0.0
  attn_drop_rate: 0.0
  drop_path_rate: 0.06666666666666667
  norm_eps: 1e-6
  shared_aln: false
  cond_drop_rate: 0.1
  attn_l2_norm: true
  patch_nums: [1, 2, 3, 4, 5, 6, 8, 10, 13, 16]
  flash_if_available: true
  fused_if_available: true
iters_train: 1563
patch_nums: [1, 2, 3, 4, 5, 6, 8, 10, 13, 16]
label_smooth: 0.0
vae_config_path: checkpoints/var/vae_ch160v4096z32.pth
# compile model for faster training with pytorch 2.0
compile: false
